{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lgb(X_fit, y_fit, X_val, y_val, counter, lgb_path, name):\n",
    "    \n",
    "    model = lgb.LGBMClassifier(max_depth=-1,\n",
    "                               n_estimators=999999,\n",
    "                               learning_rate=0.1,\n",
    "                               colsample_bytree=0.79,\n",
    "                               num_leaves=2,\n",
    "                               metric='auc',\n",
    "                               objective='binary', \n",
    "                               n_jobs=-1,\n",
    "                              verbose=2)\n",
    "     \n",
    "    model.fit(X_fit, y_fit, \n",
    "              eval_set=[(X_val, y_val)],\n",
    "              verbose=0, \n",
    "              early_stopping_rounds=1000)\n",
    "                  \n",
    "    cv_val = model.predict_proba(X_val)[:,1]\n",
    "    \n",
    "    #Save LightGBM Model\n",
    "    save_to = '{}{}_fold{}.txt'.format(lgb_path, name, counter+1)\n",
    "    model.booster_.save_model(save_to)\n",
    "    \n",
    "    return cv_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cb(X_fit, y_fit, X_val, y_val, counter, cb_path, name):\n",
    "    \n",
    "    model = cb.CatBoostClassifier(iterations=999999,\n",
    "                                  max_depth=2,\n",
    "                                  learning_rate=0.1,\n",
    "                                  colsample_bylevel=0.03,\n",
    "                                  objective=\"Logloss\",\n",
    "                                 verbose=2)\n",
    "                                  \n",
    "    model.fit(X_fit, y_fit, \n",
    "              eval_set=[(X_val, y_val)], \n",
    "              verbose=0, early_stopping_rounds=1000)\n",
    "              \n",
    "    cv_val = model.predict_proba(X_val)[:,1]\n",
    "    \n",
    "    #Save Catboost Model          \n",
    "    save_to = \"{}{}_fold{}.mlmodel\".format(cb_path, name, counter+1)\n",
    "    model.save_model(save_to, format=\"coreml\", \n",
    "                     export_parameters={'prediction_type': 'probability'})\n",
    "                     \n",
    "    return cv_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage(df_path, lgb_path, cb_path):\n",
    "    \n",
    "    print('Load Train Data.')\n",
    "    df = pd.read_csv(df_path)\n",
    "    print('\\nShape of Train Data: {}'.format(df.shape))\n",
    "    \n",
    "    y_df = np.array(df['target'])                        \n",
    "    df_ids = np.array(df.index)                     \n",
    "    df.drop(['ID_code', 'target'], axis=1, inplace=True)\n",
    "    \n",
    "    lgb_cv_result = np.zeros(df.shape[0])\n",
    "    cb_cv_result  = np.zeros(df.shape[0])\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    skf.get_n_splits(df_ids, y_df)\n",
    "    \n",
    "    print('\\nModel Fitting...')\n",
    "    for counter, ids in enumerate(skf.split(df_ids, y_df)):\n",
    "        print('\\nFold {}'.format(counter+1))\n",
    "        X_fit, y_fit = df.values[ids[0]], y_df[ids[0]]\n",
    "        X_val, y_val = df.values[ids[1]], y_df[ids[1]]\n",
    "    \n",
    "        print('LigthGBM')\n",
    "        lgb_cv_result[ids[1]] += fit_lgb(X_fit, y_fit, X_val, y_val, counter, lgb_path, name='lgb')\n",
    "        print('CatBoost')\n",
    "        cb_cv_result[ids[1]]  += fit_cb(X_fit,  y_fit, X_val, y_val, counter, cb_path,  name='cb')\n",
    "        \n",
    "        del X_fit, X_val, y_fit, y_val\n",
    "        gc.collect()\n",
    "    \n",
    "    auc_lgb  = round(roc_auc_score(y_df, lgb_cv_result),4)\n",
    "    auc_cb   = round(roc_auc_score(y_df, cb_cv_result), 4)\n",
    "    auc_mean_lgb_cb = round(roc_auc_score(y_df, (lgb_cv_result+cb_cv_result)/2), 4)\n",
    "    print('\\nLightGBM VAL AUC: {}'.format(auc_lgb))\n",
    "    print('Catboost VAL AUC: {}'.format(auc_cb))\n",
    "    print('Mean Catboost+LightGBM VAL AUC: {}'.format(auc_mean_lgb_cb))\n",
    "    print('Mean XGBoost+Catboost+LightGBM, VAL AUC: {}\\n'.format(auc_mean))\n",
    "    \n",
    "    return 0\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_stage(df_path, lgb_path, cb_path):\n",
    "    \n",
    "    print('Load Test Data.')\n",
    "    df = pd.read_csv(df_path)\n",
    "    print('\\nShape of Test Data: {}'.format(df.shape))\n",
    "    \n",
    "    df.drop(['ID_code'], axis=1, inplace=True)\n",
    "    \n",
    "    lgb_models = sorted(os.listdir(lgb_path))\n",
    "    cb_models  = sorted(os.listdir(cb_path))\n",
    "    \n",
    "    lgb_result = np.zeros(df.shape[0])\n",
    "    cb_result  = np.zeros(df.shape[0])\n",
    "    \n",
    "    print('\\nMake predictions...\\n')\n",
    "    \n",
    "    print('With LightGBM...')\n",
    "    for m_name in lgb_models:\n",
    "        #Load LightGBM Model\n",
    "        model = lgb.Booster(model_file='{}{}'.format(lgb_path, m_name))\n",
    "        lgb_result += model.predict(df.values)\n",
    "    \n",
    "    print('With CatBoost...')        \n",
    "    for m_name in cb_models:\n",
    "        #Load Catboost Model\n",
    "        model = cb.CatBoostClassifier()\n",
    "        model = model.load_model('{}{}'.format(cb_path, m_name), format = 'coreml')\n",
    "        cb_result += model.predict(df.values, prediction_type='Probability')[:,1]\n",
    "    \n",
    "    lgb_result /= len(lgb_models)\n",
    "    cb_result  /= len(cb_models)\n",
    "    \n",
    "    submission = pd.read_csv('./data/sample_submission.csv')\n",
    "    submission['target'] = ((lgb_result+cb_result)+cb_result)/3\n",
    "    submission.to_csv('lgb_cb2_starter_submission.csv', index=False)\n",
    "    submission['target'] = ((lgb_result+cb_result)+lgb_result)/3\n",
    "    submission.to_csv('lgb2_cb_starter_submission.csv', index=False)\n",
    "    submission['target'] = (lgb_result+cb_result)/2\n",
    "    submission.to_csv('lgb_cb_starter_submission.csv', index=False)\n",
    "    submission['target'] = lgb_result\n",
    "    submission.to_csv('lgb_starter_submission.csv', index=False)\n",
    "    submission['target'] = cb_result\n",
    "    submission.to_csv('cb_starter_submission.csv', index=False)\n",
    "    \n",
    "    return 0\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Stage.\n",
      "\n",
      "Load Test Data.\n",
      "\n",
      "Shape of Test Data: (200000, 201)\n",
      "\n",
      "Make predictions...\n",
      "\n",
      "With LightGBM...\n",
      "With CatBoost...\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    train_path = './data/train.csv'\n",
    "    test_path  = './data/test.csv'\n",
    "    \n",
    "    lgb_path = './lgb_models_stack/'\n",
    "    cb_path  = './cb_models_stack/'\n",
    "\n",
    "    \n",
    "    print('Prediction Stage.\\n')\n",
    "    prediction_stage(test_path, lgb_path, cb_path)\n",
    "    \n",
    "    print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import pprint\n",
    "import joblib\n",
    "\n",
    "# Suppressing warnings because of skopt verbosity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Our example dataset\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Hyperparameters distributions\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, mean_absolute_error\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Skopt functions\n",
    "from skopt import BayesSearchCV\n",
    "from skopt import gp_minimize # Bayesian optimization using Gaussian Processes\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args # decorator to convert a list of parameters to named arguments\n",
    "from skopt.callbacks import DeadlineStopper # Stop the optimization before running out of a fixed budget of time.\n",
    "from skopt.callbacks import VerboseCallback # Callback to control the verbosity\n",
    "from skopt.callbacks import DeltaXStopper # Stop the optimization If the last two positions at which the objective has been evaluated are less than delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_perf(optimizer, X, y, title, callbacks=None):\n",
    "    \"\"\"\n",
    "    A wrapper for measuring time and performances of different optmizers\n",
    "    \n",
    "    optimizer = a sklearn or a skopt optimizer\n",
    "    X = the training set \n",
    "    y = our target\n",
    "    title = a string label for the experiment\n",
    "    \"\"\"\n",
    "    start = time()\n",
    "    if callbacks:\n",
    "        optimizer.fit(X, y, callback=callbacks)\n",
    "    else:\n",
    "        optimizer.fit(X, y)\n",
    "    best_score = optimizer.best_score_\n",
    "    best_score_std = optimizer.cv_results_['std_test_score'][optimizer.best_index_]\n",
    "    best_params = optimizer.best_params_\n",
    "    print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n",
    "           +u\"\\u00B1\"+\" %.3f\") % (time() - start, \n",
    "                                  len(optimizer.cv_results_['params']),\n",
    "                                  best_score,\n",
    "                                  best_score_std))    \n",
    "    print('Best parameters:')\n",
    "    pprint.pprint(best_params)\n",
    "    print()\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting average precision score into a scorer suitable for model selection\n",
    "avg_prec = make_scorer(average_precision_score, greater_is_better=True, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "\n",
    "# Setting a 5-fold stratified cross-validation (note: shuffle=True)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# A Scikit-learn GBM classifier\n",
    "clf = GradientBoostingClassifier(n_estimators=20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Train Data.\n",
      "\n",
      "Shape of Train Data: (200000, 202)\n"
     ]
    }
   ],
   "source": [
    "print('Load Train Data.')\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "print('\\nShape of Train Data: {}'.format(df.shape))\n",
    "\n",
    "y_df = np.array(df['target'])                        \n",
    "df_ids = np.array(df.index)                     \n",
    "df.drop(['ID_code', 'target'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GridSearchCV needs a predefined plan of the experiments\n",
    "# grid_search = GridSearchCV(clf, \n",
    "#                            param_grid={\"learning_rate\": [0.01, 1.0],\n",
    "#                                        \"n_estimators\": [10, 500],\n",
    "#                                        \"subsample\": [1.0, 0.5],\n",
    "#                                        \"min_samples_split\": [2, 10],\n",
    "#                                        \"min_samples_leaf\": [1, 10],\n",
    "#                                        \"max_features\": ['sqrt', 'log2', None]\n",
    "#                                        },\n",
    "#                            n_jobs=-1,\n",
    "#                            cv=skf,\n",
    "#                            scoring=avg_prec,\n",
    "#                            iid=False, # just return the average score across folds\n",
    "#                            return_train_score=False)\n",
    "\n",
    "# best_params = report_perf(grid_search, df, y_df,'GridSearchCV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RandomizedSearchCV needs the distribution of the experiments to be tested\n",
    "# # If you can provide the right distribution, the sampling will lead to faster and better results.\n",
    "\n",
    "# random_search = RandomizedSearchCV(clf, \n",
    "#                            param_distributions={\"learning_rate\": uniform(0.01, 1.0),\n",
    "#                                                 \"n_estimators\": randint(10, 500),\n",
    "#                                                 \"subsample\": uniform(0.5, 0.5),\n",
    "#                                                 \"min_samples_split\": randint(2, 10),\n",
    "#                                                 \"min_samples_leaf\": randint(1, 10),\n",
    "#                                                 \"max_features\": ['sqrt', 'log2', None]\n",
    "#                                        },\n",
    "#                                    n_iter=40,\n",
    "#                                    cv=skf,\n",
    "#                                    scoring=avg_prec,\n",
    "#                                    iid=False, # just return the average score across folds\n",
    "#                                    return_train_score=False,\n",
    "#                                    random_state=0,\n",
    "#                                    n_jobs=2,\n",
    "#                                   verbose=10)\n",
    "\n",
    "# best_params = report_perf(random_search, df, y_df, 'RandomizedSearchCV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "clf = lgb.LGBMClassifier(boosting_type='gbdt',\n",
    "                         class_weight='balanced',\n",
    "                         objective='binary',\n",
    "                         n_jobs=1, \n",
    "                         verbose=0)\n",
    "\n",
    "search_spaces = {\n",
    "        'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
    "        'num_leaves': Integer(2, 500),\n",
    "        'max_depth': Integer(0, 500),\n",
    "        'min_child_samples': Integer(0, 200),\n",
    "        'max_bin': Integer(100, 100000),\n",
    "        'subsample': Real(0.01, 1.0, 'uniform'),\n",
    "        'subsample_freq': Integer(0, 10),\n",
    "        'colsample_bytree': Real(0.01, 1.0, 'uniform'),\n",
    "        'min_child_weight': Integer(0, 10),\n",
    "        'subsample_for_bin': Integer(100000, 500000),\n",
    "        'reg_lambda': Real(1e-9, 1000, 'log-uniform'),\n",
    "        'reg_alpha': Real(1e-9, 1.0, 'log-uniform'),\n",
    "        'scale_pos_weight': Real(1e-6, 500, 'log-uniform'),\n",
    "        'n_estimators': Integer(10, 10000)        \n",
    "        }\n",
    "\n",
    "opt = BayesSearchCV(clf,\n",
    "                    search_spaces,\n",
    "                    scoring=avg_prec,\n",
    "                    cv=skf,\n",
    "                    n_iter=40,\n",
    "                    n_jobs=4,\n",
    "                    return_train_score=False,\n",
    "                    refit=True,\n",
    "                    optimizer_kwargs={'base_estimator': 'GP'},\n",
    "                    random_state=22,\n",
    "                   verbose=10)\n",
    "    \n",
    "best_params = report_perf(opt, df, y_df,'LightGBM', \n",
    "                          callbacks=[DeltaXStopper(0.001), \n",
    "                                     DeadlineStopper(60*5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
